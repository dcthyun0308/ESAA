{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2LmFjSOB1IjOJwhRAyO/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcthyun0308/ESAA/blob/main/ESAA_YB_week12_2F_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **난독화된 한글 리뷰 복원 및 생성 AI 경진대회**\n",
        "\n",
        "# 1. 주제 및 데이터\n",
        "주제\n",
        "- 식별하기 어렵게 쓴 한글 리뷰를 원래 한글 리뷰로 복원하는 AI 알고리즘 개발\n",
        "\n",
        "데이터\n",
        "train.csv\n",
        "- ID : 리뷰의 고유 ID\n",
        "- input : 난독화된 한글 리뷰\n",
        "- output : 원본 한글 리뷰\n",
        "\n",
        "test.csv\n",
        "- ID : 리뷰의 고유 ID\n",
        "- input : 난독화된 한글 리뷰\n",
        "\n",
        "sample_submission.csv - 제출 양식\n",
        "- ID : 리뷰의 고유 ID\n",
        "- output : 복원된 한글 리뷰\n",
        "\n",
        "# 2. 코드 리뷰\n",
        "EDA\n",
        "- 빈도 기반 확률 행렬 생성 : 초성, 중성, 종성의 조건부 빈도를 저장하는 행렬 생성\n",
        "- 난독화 해체\n",
        "- 토큰화\n",
        "- 데이터 증강\n",
        "\n",
        "전처리\n",
        "- 1차 해독 : MLP를 통한 문자 수준의 해독 -> 자모 분리 + 위치 정보 계산, 임베딩, MLP LAYER\n",
        "\n",
        "- 2차 해독 : ELECTRA\n",
        "  - 전처리, Chunking\n",
        "  - SentencePiece 기반 Custom Tokenizer 학습\n",
        "  - KoELECTRA 기반 문장 복원 모델 학습\n",
        "\n",
        "모델링\n",
        "- 앙상블 : SeqMatcher, llama 앙상블 진행\n",
        "  - llama (LLM): koGemma와 유사하게, 2차 해독 단계에서 생성된 결과에 대해 더 넓은 범위의 문맥적 일관성을 부여하거나, 다양한 관점의 복원 결과(Diversity)를 제공하기 위해 활용됨.\n",
        "\n",
        "  - SeqMatcher: 서로 다른 모델(electra, koGemma, llama)의 출력이 나왔을 때, 이들 시퀀스(Sequence)들을 비교하여 가장 정확한 최종 시퀀스를 결정하는 후처리(Post-processing) 또는 가중치 부여 역할을 수행함.\n",
        "\n",
        "# 3. 차별 점 및 배울 점\n",
        "차별 점\n",
        "- 효율적인 Multi-stage 모델링\n",
        "- 태스크 특화된 LLM/PLM 조합\n",
        "- 시쿼스 기반의 정교한 앙상블\n",
        "\n",
        "배울 점\n",
        "- 모델링 전 전처리 과정의 분해 능력\n",
        "- 데이터 흐름 및 설계의 중요성\n",
        "- 후처리 및 앙상블의 완성도\n"
      ],
      "metadata": {
        "id": "Ta-VCpjxXxpT"
      }
    }
  ]
}